\documentclass[11pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{caption}
\usepackage{subcaption}

\geometry{margin=1in}

\title{Comparative Analysis of Deterministic Simple Polygon Triangulation Algorithms:\\From $O(n^2)$ to $O(n + r \log r)$}
\author{Research Report}
\date{\today}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}

\begin{document}

\maketitle

\begin{abstract}
This paper presents a comprehensive empirical comparison of three deterministic polygon triangulation algorithms with different theoretical complexities: Ear Clipping ($O(n^2)$), Garey-Johnson-Preparata-Tarjan ($O(n \log n)$), and Hertel-Mehlhorn ($O(n + r \log r)$, where $r$ is the number of reflex vertices). Our experimental evaluation covers convex, random, spiral, and star-shaped polygons with sizes ranging from 10 to 10,000 vertices. We derive empirical scaling laws $T = a \cdot n^b$ and extrapolate performance to polygons with millions of vertices. Results reveal a striking gap between theoretical complexity and practical performance: the $O(n^2)$ Ear Clipping algorithm exhibits near-linear empirical scaling ($b \approx 1.14$) and outperforms theoretically superior algorithms by 2--3 orders of magnitude. At 1 million vertices, Earcut would complete in under 1 second while Garey and Hertel would require hours.
\end{abstract}

\section{Introduction}

Polygon triangulation is a fundamental problem in computational geometry with applications in computer graphics, mesh generation, geographic information systems, and robotics. Given a simple polygon $P$ with $n$ vertices, the goal is to partition its interior into $n-2$ non-overlapping triangles using only diagonals of the polygon.

The theoretical landscape of polygon triangulation spans several complexity classes:
\begin{itemize}
    \item \textbf{$O(n^2)$}: Ear Clipping \cite{meisters1975}---iteratively remove ``ears'' (convex vertex triangles).
    \item \textbf{$O(n \log n)$}: Garey et al.\ \cite{garey1978}---monotone decomposition via sweep line.
    \item \textbf{$O(n + r \log r)$}: Hertel-Mehlhorn \cite{hertel1985}---output-sensitive, where $r$ is the reflex vertex count.
    \item \textbf{$O(n \log^* n)$}: Seidel \cite{seidel1991}---randomized trapezoidal decomposition.
    \item \textbf{$O(n)$}: Chazelle \cite{chazelle1991}---optimal but impractical due to complexity.
\end{itemize}

While theoretical complexities suggest clear preferences, practical performance depends on implementation details, constant factors, memory access patterns, and input characteristics. This work provides an empirical evaluation of three deterministic algorithms, derives their empirical scaling laws, and extrapolates performance to large-scale inputs.

\section{Algorithms}

\subsection{Ear Clipping ($O(n^2)$)}

The Ear Clipping algorithm \cite{meisters1975} is based on the two-ears theorem: every simple polygon with more than three vertices has at least two ``ears''---triangles formed by three consecutive vertices where the middle vertex is convex and no other polygon vertices lie inside.

\begin{algorithm}
\caption{Ear Clipping}
\begin{algorithmic}[1]
\Require Simple polygon $P$ with vertices $v_0, v_1, \ldots, v_{n-1}$
\Ensure Triangulation of $P$
\State $T \gets \emptyset$
\While{$|P| > 3$}
    \For{each vertex $v_i$ in $P$}
        \If{$v_i$ is an ear (convex, no vertices inside triangle)}
            \State Add triangle $(v_{i-1}, v_i, v_{i+1})$ to $T$
            \State Remove $v_i$ from $P$
            \State \textbf{break}
        \EndIf
    \EndFor
\EndWhile
\State Add remaining triangle to $T$
\State \Return $T$
\end{algorithmic}
\end{algorithm}

The worst-case complexity is $O(n^2)$ since each ear removal may require $O(n)$ time to verify, and there are $n-2$ ears to remove. However, practical implementations use linked lists and spatial hashing to achieve near-linear performance on typical inputs.

\subsection{Garey-Johnson-Preparata-Tarjan ($O(n \log n)$)}

The Garey algorithm \cite{garey1978} decomposes the polygon into $y$-monotone pieces, each triangulable in linear time.

\begin{definition}
A polygon is \emph{$y$-monotone} if any horizontal line intersects its boundary at most twice.
\end{definition}

The algorithm proceeds in two phases:
\begin{enumerate}
    \item \textbf{Monotone Decomposition}: A sweep line processes vertices top-to-bottom, adding diagonals to eliminate ``split'' and ``merge'' vertices. This produces $O(n)$ monotone subpolygons.
    \item \textbf{Monotone Triangulation}: Each monotone polygon is triangulated in linear time using a stack-based algorithm that processes vertices in sorted order.
\end{enumerate}

The overall complexity is $O(n \log n)$ due to vertex sorting in the sweep line phase.

\subsection{Hertel-Mehlhorn ($O(n + r \log r)$)}

The Hertel-Mehlhorn algorithm \cite{hertel1985} is output-sensitive with complexity $O(n + r \log r)$, where $r$ is the number of reflex vertices.

\begin{definition}
A vertex $v_i$ of polygon $P$ is \emph{reflex} if the interior angle at $v_i$ exceeds $\pi$ radians.
\end{definition}

Key properties:
\begin{itemize}
    \item For convex polygons ($r = 0$): runs in $O(n)$ time.
    \item For star polygons ($r \approx n/2$): degrades to $O(n \log n)$.
\end{itemize}

The algorithm identifies reflex vertices, sorts them, and processes them using a sweep-line approach that adds horizontal chords to decompose the polygon into $y$-monotone regions.

\section{Experimental Setup}

\subsection{Implementations}

We used the following implementations:
\begin{itemize}
    \item \textbf{Ear Clipping}: Mapbox earcut.hpp---a production-quality C++ header-only library with spatial hashing optimizations.
    \item \textbf{Garey}: Python implementation based on the GeomAlgLib \cite{geomalg} monotone decomposition algorithm.
    \item \textbf{Hertel-Mehlhorn}: Python implementation from the Fast-Triangulation-Of-The-Plane repository \cite{hertelrepo}.
\end{itemize}

All tests were run on Ubuntu Linux with GCC 11 (C++) and Python 3.10.

\subsection{Test Polygons}

Four polygon types were generated to test different algorithmic behaviors:
\begin{enumerate}
    \item \textbf{Convex}: Regular $n$-gons ($r = 0$ reflex vertices). Tests baseline performance.
    \item \textbf{Random}: Star-shaped polygons with random radii. Moderate reflex count.
    \item \textbf{Spiral}: Spiral-shaped polygons. Tests handling of elongated shapes.
    \item \textbf{Star}: Alternating inner/outer vertices ($r \approx n/2$). Maximum reflex vertices.
\end{enumerate}

Polygon sizes: $n \in \{10, 50, 100, 500, 1000, 2000, 5000, 10000\}$.

\section{Results}

\subsection{Performance Comparison}

Table~\ref{tab:benchmark_results} shows execution times for selected algorithm-polygon combinations.

\input{benchmark_table}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/benchmark_times.png}
    \caption{Algorithm performance comparison (log-log scale). Despite $O(n^2)$ complexity, Ear Clipping dominates across all sizes tested up to 10,000 vertices.}
    \label{fig:benchmark}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/benchmark_by_type.png}
    \caption{Performance breakdown by polygon type. The gap between Ear Clipping and other algorithms is consistent across polygon types.}
    \label{fig:by_type}
\end{figure}

\subsection{Scaling Law Analysis}

To understand how algorithms scale to larger inputs, we fit the empirical data to the power law model:
\begin{equation}
T(n) = a \cdot n^b
\end{equation}
where $a$ is a constant factor and $b$ is the scaling exponent. The exponent $b$ characterizes the algorithm's practical complexity independent of implementation language and constant factors.

\input{scaling_table}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/scaling_analysis.png}
    \caption{Scaling law analysis. Left: Empirical data with fitted power-law curves. Right: Comparison of scaling exponents $b$ in $T \sim n^b$.}
    \label{fig:scaling}
\end{figure}

Key findings from the scaling analysis:
\begin{enumerate}
    \item \textbf{Ear Clipping}: Empirical exponent $b = 1.14$, far below the theoretical $O(n^2)$ bound. This near-linear scaling is due to:
    \begin{itemize}
        \item Spatial hashing enabling $O(1)$ average-case point-in-triangle tests
        \item Cache-efficient linked list traversal
        \item Early termination when ears are found quickly
    \end{itemize}
    
    \item \textbf{Garey}: Empirical exponent $b = 1.73$, worse than the theoretical $O(n \log n)$ expectation. The super-linear behavior stems from:
    \begin{itemize}
        \item Python interpreter overhead
        \item Inefficient data structure operations
        \item Memory allocation costs
    \end{itemize}
    
    \item \textbf{Hertel-Mehlhorn}: Empirical exponent $b = 1.89$, approaching quadratic despite theoretical $O(n + r \log r)$. The degradation is due to implementation overhead in the sweep-line structure.
\end{enumerate}

\subsection{Extrapolation to Large Polygons}

Using the fitted scaling laws, we extrapolate performance to polygons with thousands to millions of vertices (Figure~\ref{fig:extrapolation}).

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/extrapolation.png}
    \caption{Extrapolated performance from measured data (solid points) to millions of vertices. Horizontal lines mark 1 second, 1 minute, and 1 hour thresholds.}
    \label{fig:extrapolation}
\end{figure}

\begin{table}[htbp]
\centering
\caption{Extrapolated Performance Estimates}
\label{tab:extrapolation}
\begin{tabular}{lcccc}
\toprule
Algorithm & 1K vertices & 10K vertices & 100K vertices & 1M vertices \\
\midrule
Ear Clipping & 0.3 ms & 4.4 ms & 60 ms & 0.8 s \\
Garey & 131 ms & 7.0 s & 6.3 min & 5.6 hr \\
Hertel-Mehlhorn & 244 ms & 18.7 s & 24.0 min & 30.6 hr \\
\bottomrule
\end{tabular}
\end{table}

The extrapolation reveals:
\begin{itemize}
    \item At 1 million vertices, Earcut would complete in under 1 second
    \item Garey would require approximately 5.6 hours
    \item Hertel-Mehlhorn would require over 30 hours
    \item The performance gap \emph{increases} with polygon size due to different scaling exponents
\end{itemize}

\subsection{Key Findings}

\begin{enumerate}
    \item \textbf{Theoretical complexity is misleading}: The $O(n^2)$ algorithm outperforms $O(n \log n)$ and $O(n + r \log r)$ algorithms by 2--3 orders of magnitude.
    
    \item \textbf{Empirical scaling differs from theory}:
    \begin{itemize}
        \item Earcut: $b = 1.14$ (theory: $b = 2$)
        \item Garey: $b = 1.73$ (theory: $b \approx 1$)
        \item Hertel: $b = 1.89$ (theory: $b \approx 1$)
    \end{itemize}
    
    \item \textbf{Implementation quality dominates}: A well-optimized C++ implementation beats unoptimized Python implementations regardless of algorithmic complexity.
    
    \item \textbf{The gap widens at scale}: Due to better scaling exponents, Earcut's advantage increases from 100x at $n=1000$ to 10,000x at $n=1,000,000$.
\end{enumerate}

\section{Discussion}

\subsection{Theory vs.\ Practice}

Our results highlight a fundamental tension in algorithm design:

\begin{quote}
\emph{Asymptotic complexity describes behavior as $n \to \infty$, but practical applications operate in finite regimes where constant factors and implementation quality dominate.}
\end{quote}

The crossover point where $O(n \log n)$ beats $O(n^2)$ depends on:
\begin{enumerate}
    \item The constant factors $a$ in $T = a \cdot f(n)$
    \item The actual scaling behavior (which may differ from theoretical bounds)
    \item Implementation language and optimization level
\end{enumerate}

For polygon triangulation, our data suggests the crossover would occur at $n > 10^8$ vertices---far beyond typical use cases.

\subsection{Practical Recommendations}

Based on our experiments:
\begin{itemize}
    \item \textbf{For any practical $n$}: Use Ear Clipping (Earcut). It is faster, simpler, and more robust.
    \item \textbf{For $n > 10^6$}: Consider optimized C++ implementations of Garey or Seidel's algorithm.
    \item \textbf{For mesh quality}: Consider Constrained Delaunay Triangulation which produces better-shaped triangles.
    \item \textbf{Never trust complexity alone}: Always benchmark with representative inputs.
\end{itemize}

\subsection{Limitations}

This study has several limitations:
\begin{itemize}
    \item Language mismatch (C++ vs.\ Python) affects comparison fairness.
    \item Python implementations may not be fully optimized.
    \item Extrapolation assumes power-law scaling continues (may not hold for very large $n$).
    \item Memory effects not measured (could become significant for $n > 10^6$).
\end{itemize}

\section{Conclusion}

We presented an empirical comparison of three deterministic polygon triangulation algorithms: Ear Clipping ($O(n^2)$), Garey ($O(n \log n)$), and Hertel-Mehlhorn ($O(n + r \log r)$). 

Our key findings are:
\begin{enumerate}
    \item \textbf{Theoretical complexity is a poor predictor of practical performance.}
    \item The simple $O(n^2)$ Ear Clipping algorithm, when well-implemented, exhibits near-linear empirical scaling ($b \approx 1.14$).
    \item At 1 million vertices, Earcut would be approximately 25,000x faster than Garey and 140,000x faster than Hertel-Mehlhorn.
    \item Implementation quality and constant factors dominate algorithmic complexity for practical input sizes.
\end{enumerate}

This result underscores the importance of empirical benchmarking alongside theoretical analysis when selecting algorithms for real-world applications.

\bibliographystyle{plain}
\begin{thebibliography}{10}

\bibitem{chazelle1991}
B.~Chazelle.
\newblock Triangulating a simple polygon in linear time.
\newblock {\em Discrete \& Computational Geometry}, 6(5):485--524, 1991.

\bibitem{garey1978}
M.~R. Garey, D.~S. Johnson, F.~P. Preparata, and R.~E. Tarjan.
\newblock Triangulating a simple polygon.
\newblock {\em Information Processing Letters}, 7(4):175--179, 1978.

\bibitem{hertel1985}
S.~Hertel and K.~Mehlhorn.
\newblock Fast triangulation of the plane with respect to simple polygons.
\newblock {\em Information and Control}, 64(1-3):52--76, 1985.

\bibitem{meisters1975}
G.~H. Meisters.
\newblock Polygons have ears.
\newblock {\em The American Mathematical Monthly}, 82(6):648--651, 1975.

\bibitem{seidel1991}
R.~Seidel.
\newblock A simple and fast incremental randomized algorithm for computing
  trapezoidal decompositions and for triangulating polygons.
\newblock {\em Computational Geometry: Theory and Applications}, 1(1):51--64, 1991.

\bibitem{geomalg}
J.~Dinkla.
\newblock GeomAlgLib: Geometric Algorithms in Haskell.
\newblock \url{https://github.com/jdinkla/GeomAlgLib}, 1998.

\bibitem{hertelrepo}
A.~Porfanid.
\newblock Fast-Triangulation-Of-The-Plane.
\newblock \url{https://github.com/porfanid/Fast-Triangulation-Of-The-Plane}, 2024.

\bibitem{earcut}
Mapbox.
\newblock Earcut: A C++ port of earcut.js.
\newblock \url{https://github.com/mapbox/earcut.hpp}, 2023.

\end{thebibliography}

\end{document}
